{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a>\n",
    "\n",
    "# Importing the Dataset\n",
    "\n",
    "\n",
    "                                                   By: Jun Ho Lee\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Table of Contents'></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. <a href='#Introduction'>Introduction</a>\n",
    "2. <a href='#Import Dataset'>Importing the Dataset</a>\n",
    "3. <a href='#Memory Reduction'>Memory Reduction</a>\n",
    "4. <a href='#JSON Format'>JSON Format</a>\n",
    "\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Introduction'></a>\n",
    "***\n",
    "\n",
    "\n",
    "### 1. Introduction \n",
    "\n",
    "The dataset I will be using comes from **United States Department of Labor on Foreign Labor Certification.** [Dataset Link](https://www.foreignlaborcert.doleta.gov/performancedata.cfm)  \n",
    "\n",
    "The dataset is from Fiscal Year 2018 with a reporting period from October 1, 2017 through September 30, 2018. To access the dataset from the URL provided above, click on the Disclosure Data tab and download the ‘H-1B_FY2018.xlsx’ dataset under LCA Programs (H-1B, H-1B1, E-3). The dataset dictionary is downloadable under the File Structure column. (see blue box)\n",
    "\n",
    "![](imgs/data_boxed_small.png)\n",
    "\n",
    "\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Import Dataset'></a>\n",
    "___\n",
    "\n",
    "### 2. Importing the Dataset\n",
    "\n",
    "The original downloaded file is in .xlsx extension. Pandas do [support](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) both .xls and .xlsx file extensions, but the performance is notoriously [slow](https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe)! Reading in CSV format is much faster and luckily, we can save the excel file into a csv format. Thus, we will use pandas.read_csv() to read in the converted CSV format. \n",
    "\n",
    "* **Don't actually read in the csv file! (Reason explained <a href='#Reason'>below</a>)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Import necessry libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Read in csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1b_df = pd.read_csv(\"data/H-1B_Disclosure_Data_FY2018_EOY.csv\", low_memory=False)\n",
    "# h1b_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Reason'></a>\n",
    "**C. Reason**  \n",
    "\n",
    "Even though .read_csv() is relatively faster than .read_excel(), this operation will eat up your RAM and slow down your computer performance. If we actually check the memory usage for loading in this dataset, it is actually around ~1.6GB, which can be quite costly, especially for users with low RAM. This particular dataset was only originally around ~200MB but if you imagine blindly running .read_csv() on a larger dataset, it might crash your computer. To prevent these issues, it is essential to reduce memory usage when reading in a particularly large dataset. \n",
    "\n",
    "\n",
    "Extensive RAM usage occurs because python by default reads in columns using the highest bytesize `dtype` available (`float64`, `int64` etc.) `String` type is the most costly, since the length of the `String` is proportional to its memory usage. We can solve this problem by downcasting `float64` and `int64` and converting `String` into categoricals. \n",
    "\n",
    "<a href='#Top'>Back to Top</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Memory Reduction'></a>\n",
    "***\n",
    "\n",
    "### 3. Memory Reduction\n",
    "\n",
    "- **Necessity:** Prevent potential computer crashes when reading in large datasets. \n",
    "\n",
    "\n",
    "- **Objective:** Reduce memory usage by downcasting numerical values and converting string objects into categorical values whenever feasible. Use the converted datatypes as parameters for reading in the dataset.  \n",
    "\n",
    "\n",
    "- **Method:** \n",
    "    1. Downcast `int64` \n",
    "    2. Downcast `float64` \n",
    "    3. Convert `object` (strings) to `categoricals`\n",
    "\n",
    "\n",
    "- **Reference:** [Dataquest](https://www.dataquest.io/blog/pandas-big-data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A. Before Getting Started:*** Make a Custom Function to check Memory Usage \n",
    "- Will be useful to determine the memory usage of the final optimized dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj, pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Read in first n lines of csv file to determine dtypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000  # keep n high to capture as many unique values as possible\n",
    "df = pd.read_csv(\"data/H-1B_Disclosure_Data_FY2018_EOY.csv\", nrows=n)\n",
    "\n",
    "# # Drop columns of mixed type \n",
    "drop_columns = ['EMPLOYER_PHONE','EMPLOYER_PHONE_EXT']\n",
    "df.drop(drop_columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Downcasting numerical data types (ints / floats)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # downcast will only work if the final downcasted version uses less memory than original\n",
    "\n",
    "# # downcasting ints\n",
    "df_int = df.select_dtypes(include=['int'])\n",
    "converted_int = df_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "# # downcasting floats\n",
    "df_float = df.select_dtypes(include=['float'])\n",
    "converted_float = df_float.apply(pd.to_numeric, downcast='float') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Converting object data types to Categoricals**\n",
    "- Note: Blind conversion of `objects` to `Categoricals` is not preferred. If the number of unique values is close to the total number of values in a column, `Categorical` type will end up using more memory! Thus it is important to initially explore the data and make a judgement call based on the number of unique values in a column. Here, I used a cutoff threshold of 50%, meaning I converted those columns where less than 50% of the values were unique values into `Categoricals`. \n",
    "- Note2: Normal string methods don't work on `Categoricals`. \n",
    "- Note3: Columns that represent dates should be converted to `datetime` for easier `datetime` operation / calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Date columns to drop. Need to retain them as object dtype \n",
    "# # (later use parse_dates WHILE reading in the dataset)\n",
    "date_columns = ['CASE_SUBMITTED','DECISION_DATE','EMPLOYMENT_START_DATE','EMPLOYMENT_END_DATE','ORIGINAL_CERT_DATE']\n",
    "\n",
    "# # Select columns with 'object' dtype\n",
    "df_obj = df.select_dtypes(include=['object'])\n",
    "\n",
    "# # Drop the 'Date' Columns since they'll have to be in datetime dtype later\n",
    "# # Otherwise, 'Date' Columns will get converted to 'Categorical'\n",
    "df_obj_wo_dates = df_obj.drop(date_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. Only convert columns where less than 50% are unique values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only convert columns where less than 50% are unique values\n",
    "\n",
    "converted_obj_wo_dates = pd.DataFrame()\n",
    "for col in df_obj_wo_dates.columns:\n",
    "    num_unique_values = len(df_obj_wo_dates[col].unique())\n",
    "    num_total_values = len(df_obj_wo_dates[col])\n",
    "    if num_unique_values / num_total_values < 0.5:  # threshold = 50%\n",
    "        converted_obj_wo_dates.loc[:,col] = df_obj_wo_dates[col].astype('category')\n",
    "    else:\n",
    "        converted_obj_wo_dates.loc[:,col] = df_obj_wo_dates[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F. Concatenate the converted dataframes into final Dataframe**\n",
    "- Final Dataframe will have dtypes of our interest (Downcasted and `Categorical`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate all the converted dataframes and evaluate memory usage\n",
    "optimized_sample = pd.concat([converted_int, converted_float, converted_obj_wo_dates], axis=1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G. Parse out the datatypes from optimized_df** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the dtypes from the optimized_sample dataframe\n",
    "dtypes = optimized_sample.dtypes\n",
    "\n",
    "col_name = dtypes.index                      # name of the column \n",
    "col_types = [i.name for i in dtypes.values]  # datatype of the column\n",
    "\n",
    "# # Save column name and types into a dictionary \n",
    "# # {name: type}\n",
    "column_types = dict(zip(col_name, col_types))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Function of the code above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_dtypes(df):\n",
    "    dtypes = df.dtypes\n",
    "\n",
    "    col_name = dtypes.index                      # name of the column \n",
    "    col_types = [i.name for i in dtypes.values]  # datatype of the column\n",
    "\n",
    "    # # Save column name and types into a dictionary \n",
    "    # # {name: type}\n",
    "    column_types = dict(zip(col_name, col_types))\n",
    "    return column_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**H. Read in the dataset with our optimized data type parameters!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.63 MB\n"
     ]
    }
   ],
   "source": [
    "final_cols = df.columns\n",
    " \n",
    "optimized_full = pd.read_csv(\"data/H-1B_Disclosure_Data_FY2018_EOY.csv\", usecols=final_cols, dtype=column_types, low_memory=False, parse_dates=date_columns,infer_datetime_format=True)\n",
    "print(mem_usage(optimized_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I. Conclusion**\n",
    "\n",
    "As you can see, the memory usage of the new optimized dataset is only 204MB! This is a 800% increase in performance when compared to initially blindly reading in the dataset. (~1.6GB used)\n",
    "\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='JSON Format'></a>\n",
    "___\n",
    "\n",
    "\n",
    "### 4. Save Parameters to JSON Format\n",
    "\n",
    "- **Objective:** Save the parameters used for reading in the optimized dataframe into a JSON file format so that other jupyter notebooks may read in the same dataset efficiently with the same parameters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**A. Make a dictionary using the original parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types  # type (dict)\n",
    "date_columns  # type (list)\n",
    "final_columns = list(df.columns) # type (list) # df.columns type=(Index) -> must convert to list \n",
    "\n",
    "t_list = [column_types, date_columns, final_columns]\n",
    "t_string = ['c_type','c_date','c_final']\n",
    "\n",
    "# Make a dictionary \n",
    "params_dict = {}\n",
    "\n",
    "for i in range(len(t_list)):\n",
    "    name = t_string[i]\n",
    "    params_dict[name] = t_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Convert dictionary to JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to json format\n",
    "import json \n",
    "\n",
    "with open('data/h1b_df_params.json', 'w') as fp:\n",
    "    json.dump(params_dict, fp)  # convert dict as a json file format! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Conclusion**\n",
    "\n",
    "- Parameters have now been saved to a JSON file in the current working directory so that other jupyter noteboks can easily access the optimized column types when reading in this particular dataset.\n",
    "\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF NOTEBOOK\n",
    "<a href='#Top'>Back to Top</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
