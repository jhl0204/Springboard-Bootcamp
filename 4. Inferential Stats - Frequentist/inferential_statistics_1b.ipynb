{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Statistics Ib - Frequentism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second Frequentist inference mini-project! Over the course of working on this mini-project and the previous frequentist mini-project, you'll learn the fundamental concepts associated with frequentist inference. The following list includes the topics you will become familiar with as you work through these two mini-projects:\n",
    "* the _z_-statistic\n",
    "* the _t_-statistic\n",
    "* the difference and relationship between the two\n",
    "* the Central Limit Theorem, its assumptions and consequences\n",
    "* how to estimate the population mean and standard deviation from a sample\n",
    "* the concept of a sampling distribution of a test statistic, particularly for the mean\n",
    "* how to combine these concepts to calculate confidence intervals and p-values\n",
    "* how those confidence intervals and p-values allow you to perform hypothesis (or A/B) tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what a random variable is\n",
    "* what a probability density function (pdf) is\n",
    "* what the cumulative density function is\n",
    "* a high-level sense of what the Normal distribution\n",
    "\n",
    "If these concepts are new to you, please take a few moments to Google these topics in order to get a sense of what they are and how you might use them.\n",
    "\n",
    "These two notebooks were designed to bridge the gap between having a basic understanding of probability and random variables and being able to apply these concepts in Python. This second frequentist inference mini-project focuses on a real-world application of this type of inference to give you further practice using these concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used only data from a known normal distribution. You'll now tackle real data, rather than simulated data, and answer some relevant real-world business problems using the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital medical charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a hospital has hired you as their data analyst. An administrator is working on the hospital's business operations plan and needs you to help them answer some business questions. This mini-project, as well as the bootstrap and Bayesian inference mini-projects also found in this unit are designed to illustrate how each of the inferential statistics methods have their uses for different use cases. In this assignment notebook, you're going to use frequentist statistical inference on a data sample to answer the questions:\n",
    "* has the hospital's revenue stream fallen below a key threshold?\n",
    "* are patients with insurance really charged different amounts than those without?\n",
    "Answering that last question with a frequentist approach makes some assumptions, or requires some knowledge, about the two groups. In the next mini-project, you'll use bootstrapping to test that assumption. And in the final mini-project of the unit, you're going to create a model for simulating _individual_ charges (not a sampling distribution) that the hospital can use to model a range of scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use some data on medical charges obtained from [Kaggle](https://www.kaggle.com/easonlai/sample-insurance-claim-prediction-dataset). For the purposes of this exercise, assume the observations are the result of random sampling from our one hospital. Recall in the previous assignment, we introduced the Central Limit Theorem (CLT), and how it tells us that the distributions of sample statistics approach a normal distribution as $n$ increases. The amazing thing about this is that it applies to the sampling distributions of statistics that have been calculated from even highly non-normal distributions of data. Remember, also, that hypothesis testing is very much based on making inferences about such sample statistics. You're going to rely heavily on the CLT to apply frequentist (parametric) tests to answer the questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t, ttest_ind_from_stats\n",
    "from numpy.random import seed\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set_style({'axes.grid' : False})\n",
    "\n",
    "medical = pd.read_csv('data/insurance2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To turn grid settings off in seaborn: \n",
    "\n",
    "- Go to this [SO Answer](https://stackoverflow.com/questions/26868304/how-to-get-rid-of-grid-lines-when-plotting-with-seaborn-pandas-with-secondary)\n",
    "- Refer [this site](http://gree2.github.io/python/2015/05/05/python-seaborn-tutorial-controlling-figure-aesthetics) as well (for aesthetics cheatkey)\n",
    "\n",
    "##### Markdown cheatsheet\n",
    "- [Click Here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>insuranceclaim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex     bmi  children  smoker  region      charges  insuranceclaim\n",
       "0   19    0  27.900         0       1       3  16884.92400               1\n",
       "1   18    1  33.770         1       0       2   1725.55230               1\n",
       "2   28    1  33.000         3       0       2   4449.46200               0\n",
       "3   33    1  22.705         0       0       1  21984.47061               0\n",
       "4   32    1  28.880         0       0       1   3866.85520               1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Plot the histogram of charges and calculate the mean and standard deviation. Comment on the appropriateness of these statistics for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ The sample does not follow normal distribution, so although the computed statistics is not wrong, we may need to interpret it with a grain of salt since it will not be the best unbiased point estimate of the population. We would need to introduce confidence intervals to make correct assumptions about the uncertainty.\n",
    "\n",
    "See this [post](https://www.researchgate.net/post/Statistics_for_non-normally_distributed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(medical.charges, bins=50) # bin number represents the number of bins\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13270.422265141257, 12110.011236694001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mean = np.mean(medical.charges)\n",
    "sample_std = np.std(medical.charges, ddof=1)\n",
    "(sample_mean, sample_std)\n",
    "\n",
    "# medical.charges.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ The administrator is concerned that the actual average charge has fallen below 12000, threatening the hospital's operational model. On the assumption that these data represent a random sample of charges, how would you justify that these data allow you to answer that question? And what would be the most appropriate frequentist test, of the ones discussed so far, to apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ The data is right skewed and is clearly not normally distributed. However, we can correctly justify our usage of t-statistic and z-statistic because the data sufficiently meets the asusmptions for CLT (Central Limit Theorem) \n",
    "\n",
    "1. The question already states that the data represents a _random sample of charges_. This satisfies the 'Randomization Condition'.\n",
    "2. Individual medical charges are _independent_ of each other since they were randomly sampled. Also in the practical world, individual medical charges are independent events. \n",
    "3. Sample size is over 1300. The sample is not normally distributed but the big sample size makes up for the non-normality. Thus 'Sample Size Assumption' is met. \n",
    "\n",
    "\n",
    "Reference: [Central Limit Theorem Assumptions and Conditions](https://cnx.org/contents/7mUmR30Q@1/Central-Limit-Theorem-Assumptions-and-Conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Given the nature of the administrator's concern, what is the appropriate confidence interval in this case? A one-sided or two-sided interval? Calculate the critical value and the relevant 95% confidence interval for the mean and comment on whether the administrator should be concerned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ One-sided interval would be more appropriate since the nature of administrator's concern is directional. The null hypothesis $H_0$ would be that we do not see a change in the sample mean (13270.422265141257). Moreover we would be calculating the left tail critical value since we want to know if the average charge has fallen below $12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The critical t value for 95% confidence interval is: -1.6459941145571324 \n"
     ]
    }
   ],
   "source": [
    "# Calculating Critical Value \n",
    "# We're testing a value (12000) below the mean - thus p would be 0.05\n",
    "p = 0.05\n",
    "sample_size = medical.shape[0]\n",
    "df = sample_size - 1  # degrees of freedom \n",
    "\n",
    "t_critical = t.ppf(p, df)\n",
    "print(\"The critical t value for 95% confidence interval is: {} \".format(t_critical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The margin of error is: -544.9350813250255 \n"
     ]
    }
   ],
   "source": [
    "# Margin of Error:\n",
    "# moe = critical_value * standard_error\n",
    "\n",
    "standard_error = sample_std / (np.sqrt(sample_size))\n",
    "margin = t_critical *  standard_error\n",
    "print(\"The margin of error is: {} \".format(margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean is: $13270.422265141257, and the corresponding value for the t-statistic of -1.645 is : $12725.48718381623\n",
      "\n",
      "\n",
      "Therefore, we can be confident 95% of the time that the true population mean lies above the value of $12725.48718381623\n",
      "\n",
      "\n",
      "In conclusion, the administrator shouldn't be too concerned about decreasing medical charges. (But maybe us patients do?! haha)\n"
     ]
    }
   ],
   "source": [
    "sample_mean\n",
    "interval_lower_bound = sample_mean + margin \n",
    "print(\"Sample mean is: ${}, and the corresponding value for the t-statistic of -1.645 is : ${}\".format(sample_mean, interval_lower_bound))\n",
    "print(\"\\n\")\n",
    "print(\"Therefore, we can be confident 95% of the time that the true population mean lies above the value of ${}\".format(interval_lower_bound))\n",
    "print(\"\\n\")\n",
    "print(\"In conclusion, the administrator shouldn't be too concerned about decreasing medical charges. (But maybe us patients do?! haha)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The administrator then wants to know whether people with insurance really are charged a different amount to those without.\n",
    "\n",
    "__Q:__ State the null and alternative hypothesis here. Use the _t_-test for the difference between means where the pooled standard deviation of the two groups is given by\n",
    "\\begin{equation}\n",
    "s_p = \\sqrt{\\frac{(n_0 - 1)s^2_0 + (n_1 - 1)s^2_1}{n_0 + n_1 - 2}}\n",
    "\\end{equation}\n",
    "\n",
    "and the *t* test statistic is then given by\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\bar{x}_0 - \\bar{x}_1}{s_p \\sqrt{1/n_0 + 1/n_1}}.\n",
    "\\end{equation}\n",
    "\n",
    "What assumption about the variances of the two groups are we making here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ Assumption: Variances of the two samples (groups) are homogenous\n",
    "\n",
    "$H_0$: there is NO difference in the amount charged between people with insurance and with people without  <br>\n",
    "$H_\\alpha$: there IS a difference in the amount charged between the two groups\n",
    "\n",
    "\n",
    "Reference: [Statistics How To](https://www.statisticshowto.datasciencecentral.com/homoscedasticity/),\n",
    "[Ohio University Study Guide pdf](https://www.ohio.edu/plantbio/staff/mccarthy/quantmet/lectures/ttest.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Perform this hypothesis test both manually, using the above formulae, and then using the appropriate function from [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-tests) (hint, you're looking for a function to perform a _t_-test on two independent samples). For the manual approach, calculate the value of the test statistic and then its probability (the p-value). Verify you get the same results from both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note to SELF (since I always tend to forget this...)\n",
    "\n",
    "- [How to get first row value of a given column](https://stackoverflow.com/questions/25254016/pandas-get-first-row-value-of-a-given-column)\n",
    "- [List of Agg functions for groupby](https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby insurance claim and find the row with(1) or without(0) insurance claims\n",
    "grouped = medical.groupby('insuranceclaim')\n",
    "\n",
    "# # # Calculating different statistics: \n",
    "\n",
    "# Calculate n (sample size) \n",
    "# (get the first value of the Series) \n",
    "n0 = grouped.count().loc[0][0]\n",
    "n1 = grouped.count().loc[1][0]\n",
    "\n",
    "# Calculate the mean\n",
    "x0 = grouped.mean().loc[0,'charges']\n",
    "x1 = grouped.mean().loc[1,'charges']\n",
    "\n",
    "\n",
    "# Calculate the standard deviation\n",
    "s0 = grouped.std().loc[0,'charges']\n",
    "s1 = grouped.std().loc[1,'charges']\n",
    "\n",
    "# x0 - x1 \n",
    "# x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "s_p = \\sqrt{\\frac{(n_0 - 1)s^2_0 + (n_1 - 1)s^2_1}{n_0 + n_1 - 2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\frac{\\bar{x}_0 - \\bar{x}_1}{s_p \\sqrt{1/n_0 + 1/n_1}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11520.034268775256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate pooled standard deviation \n",
    "# Formula (given above)\n",
    "\n",
    "std_pool = np.sqrt(((n0-1) * (s0 ** 2) + (n1-1) * (s1 ** 2)) / (n0 + n1 - 2))\n",
    "\n",
    "std_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate t-statistic\n",
    "# Formula (given above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.89329903087671"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_critical = (x0 - x1) / ((std_pool) * np.sqrt((1/n0) + (1/n1)))\n",
    "t_critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference between cdf and ppf\n",
    "\n",
    "1.CDF (Cumulative Distribution Function):\n",
    "  - argument --> critical value\n",
    "  - output --> p - value\n",
    "  \n",
    "2.PPF (Percentile Point Function):\n",
    "  - argument --> p - value (probability) \n",
    "  - output --> critical value\n",
    "\n",
    "\n",
    "\n",
    "__CDF and PPF have inverse relationships__ \n",
    "\n",
    "[Nice Summary](http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/en_Tanagra_Calcul_P_Value.pdf) on how to use scipy.stats regarding cdf and ppf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6459949688112576"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example Case\n",
    "# Cumulative Probability of the area under the curve left of critical value = 1 \n",
    "p_val_ex = t.cdf(1, n0+n1-2)\n",
    "p_val_ex\n",
    "\n",
    "# Critical Value given the cumulative probability = 0.95 for one-tailed test\n",
    "t_critical_ex = t.ppf(0.95, n0+n1-2)\n",
    "t_critical_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value given that the null hypothesis is true is: 2.230615115810486e-31\n"
     ]
    }
   ],
   "source": [
    "# Calculating the p-value using the t-statistic from above\n",
    "p_value = t.cdf(t_critical, n0+n1-2)\n",
    "p_value\n",
    "print(\"P-value given that the null hypothesis is true is: {}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scipy.stats function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-test for two independent samples (equal_var = True by default)\n",
    "\n",
    "References: \n",
    "- [ttest from samples](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind)\n",
    "- [ttest from descriptive stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html#scipy.stats.ttest_ind_from_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the scipy.stats function gives us t critical value of: -11.893299030876712 and P-value of 4.461230231620717e-31\n"
     ]
    }
   ],
   "source": [
    "# ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2, equal_var=True)\n",
    "\n",
    "t_critical_stats, p_value_stats = ttest_ind_from_stats(x0, s0, n0, x1, s1, n1)\n",
    "print(\"Using the scipy.stats function gives us t critical value of: {} and P-value of {}\".format(t_critical_stats, p_value_stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Hopefully you got the exact same numerical results. This shows that you correctly calculated the numbers by hand. Secondly, you used the correct function and saw that it's much easier to use. All you need to do pass your data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ In the above calculations, we assumed the sample variances were equal. We may well suspect they are not (we'll explore this in another assignment). The calculation becomes a little more complicated to do by hand in this case, but we now know of a helpful function. Check the documentation for the function to tell it not to assume equal variances and perform the test again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ If two groups have unequal variances (heteroscedastic), then perform Welch's t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the scipy.stats function to conduct t-test for two independent samples with unequal variances gives us t critical value of: -13.298031957975647 and P-value of 1.1105103216309438e-37\n"
     ]
    }
   ],
   "source": [
    "t_critical_welch, p_value_welch = ttest_ind_from_stats(x0, s0, n0, x1, s1, n1, equal_var=False)\n",
    "\n",
    "print(\"Using the scipy.stats function to conduct t-test for two independent samples\\\n",
    " with unequal variances gives us t critical value of: {} and P-value of {}\".format(t_critical_welch, p_value_welch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Conceptual question: look through the documentation for statistical test functions in scipy.stats. You'll see the above _t_-test for a sample, but can you see an equivalent one for performing a *z*-test from a sample? Comment on your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ I couldn't find any documentation for doing z-tests in scipy.stats but it looks like __statsmodels__ has the relevant api call for conducting z-tests [Reference](https://www.statsmodels.org/stable/generated/statsmodels.stats.weightstats.ztest.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed this project notebook, you now have good hands-on experience:\n",
    "* using the central limit theorem to help you apply frequentist techniques to answer questions that pertain to very non-normally distributed data from the real world\n",
    "* performing inference using such data to answer business questions\n",
    "* forming a hypothesis and framing the null and alternative hypotheses\n",
    "* testing this using a _t_-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
